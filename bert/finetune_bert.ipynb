{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zm2FEueu3skl"
   },
   "source": [
    "# BERT FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UmFGb9IHRNh"
   },
   "source": [
    "## Setting Up Environment \n",
    "\n",
    "**USE_TPU :-** True, If you want to use TPU runtime. First change Colab Notebook runtype to TPU\n",
    "\n",
    "**BERT_MODEL:-**\n",
    "**uncased_L-24_H-1024_A-16**: uncased BERT large model\n",
    "\n",
    "**BUCKET:-** Add bucket details, It is necessary to add bucket for TPU. For GPU runtype, If Bucket is empty, We will use disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "u6MTimx2mvKo",
    "outputId": "e3119c4e-ba86-40b6-d991-c6c047603ba5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Authenticate, so we can access storage bucket and TPU\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# If you want to use TPU, first switch to tpu runtime in colab\n",
    "USE_TPU = True #@param{type:\"boolean\"}\n",
    "\n",
    "# We will use base uncased bert model, you can give try with large models\n",
    "# For large model TPU is necessary\n",
    "BERT_MODEL = 'uncased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
    "\n",
    "# BERT checkpoint bucket\n",
    "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
    "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
    "!gsutil ls $BERT_PRETRAINED_DIR\n",
    "\n",
    "# Bucket for saving checkpoints and outputs\n",
    "BUCKET = 'quorabert123'\n",
    "if BUCKET!=\"\":\n",
    "  OUTPUT_DIR = 'gs://{}/outputs'.format(BUCKET)\n",
    "  tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "else:\n",
    "  OUTPUT_DIR = 'out_dir'\n",
    "  os.mkdir(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
    "\n",
    "if USE_TPU:\n",
    "  # getting info on TPU runtime\n",
    "  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Change notebook runtype to TPU'\n",
    "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "  print('TPU address is', TPU_ADDRESS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONIXa1_Pr1xX"
   },
   "source": [
    "## Clone BERT Repo and Download Quora Questions Pairs Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "0dTKAzm1k5BE",
    "outputId": "b57db3b8-e4c4-434d-c62a-e41048b6b695"
   },
   "outputs": [],
   "source": [
    "# Clone BERT repo and add bert in system path\n",
    "!test -d bert || git clone -q https://github.com/google-research/bert.git\n",
    "if not 'bert' in sys.path:\n",
    "  sys.path += ['bert']\n",
    "# Download QQP Task dataset present in GLUE Tasks.\n",
    "TASK_DATA_DIR = 'glue_data/QQP'\n",
    "!test -d glue_data || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git glue_data\n",
    "!test -d $TASK_DATA_DIR || python glue_data/download_glue_data.py --data_dir glue_data --tasks=QQP\n",
    "!ls -als $TASK_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nGLW4s-L6ws"
   },
   "source": [
    "## Model Configs and Hyper Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUNH1_-zHJIH"
   },
   "outputs": [],
   "source": [
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import run_classifier\n",
    "\n",
    "# Model Hyper Parameters\n",
    "TRAIN_BATCH_SIZE = 32 # For GPU, reduce to 16\n",
    "EVAL_BATCH_SIZE = 8\n",
    "PREDICT_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 2.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_SEQ_LENGTH = 200 \n",
    "\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "ITERATIONS_PER_LOOP = 1000\n",
    "NUM_TPU_CORES = 8\n",
    "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RDGdggFQpFB"
   },
   "source": [
    "## Read Questions Pairs\n",
    "\n",
    "We will read data from TSV file and covert to list of InputExample. For `InputExample` and `DataProcessor` class defination refer to [run_classifier](https://github.com/google-research/bert/blob/master/run_classifier.py) file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5RvBsrOrKLJN"
   },
   "outputs": [],
   "source": [
    "class QQPProcessor(run_classifier.DataProcessor):\n",
    "  \"\"\"Processor for the Quora Question pair data set.\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"Reading train.tsv and converting to list of InputExample\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir,\"train.tsv\")), 'train')\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"Reading dev.tsv and converting to list of InputExample\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir,\"dev.tsv\")), 'dev')\n",
    "  \n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"Reading train.tsv and converting to list of InputExample\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir,\"test.tsv\")), 'test')\n",
    "  \n",
    "  def get_predict_examples(self, sentence_pairs):\n",
    "    \"\"\"Given question pairs, conevrting to list of InputExample\"\"\"\n",
    "    examples = []\n",
    "    for (i, qpair) in enumerate(sentence_pairs):\n",
    "      guid = \"predict-%d\" % (i)\n",
    "      # converting questions to utf-8 and creating InputExamples\n",
    "      text_a = tokenization.convert_to_unicode(qpair[0])\n",
    "      text_b = tokenization.convert_to_unicode(qpair[1])\n",
    "      # We will add label  as 0, because None is not supported in converting to features\n",
    "      examples.append(\n",
    "          run_classifier.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=0))\n",
    "    return examples\n",
    "  \n",
    "  def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      guid = \"%s-%d\" % (set_type, i)\n",
    "      if set_type=='test':\n",
    "        # removing header and invalid data\n",
    "        if i == 0 or len(line)!=3:\n",
    "          print(guid, line)\n",
    "          continue\n",
    "        text_a = tokenization.convert_to_unicode(line[1])\n",
    "        text_b = tokenization.convert_to_unicode(line[2])\n",
    "        label = 0 # We will use zero for test as convert_example_to_features doesn't support None\n",
    "      else:\n",
    "        # removing header and invalid data\n",
    "        if i == 0 or len(line)!=6:\n",
    "          continue\n",
    "        text_a = tokenization.convert_to_unicode(line[3])\n",
    "        text_b = tokenization.convert_to_unicode(line[4])\n",
    "        label = int(line[5])\n",
    "      examples.append(\n",
    "          run_classifier.InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"return class labels\"\n",
    "    return [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRm65m-_ki05"
   },
   "source": [
    "## Convert to Features\n",
    "\n",
    "We will read examples and tokenize using Wordpiece based tokenization. Finally We will convert to `InputFeatures`.\n",
    "\n",
    "BERT follows below tokenization procedure\n",
    "1.   Instantiate an instance of tokenizer = tokenization.FullTokenizer\n",
    "2.   Tokenize the raw text with tokens = tokenizer.tokenize(raw_text).\n",
    "3.   Truncate to the maximum sequence length.\n",
    "4.   Add the [CLS] and [SEP] tokens in the right place.\n",
    "\n",
    "We need to create `segment_ids`, `input_mask` for `InputFeatures`. `segment_ids` will be `0` for question1 tokens and `1` for question2 tokens.\n",
    "\n",
    "We will use following functions from [run_classifier](https://github.com/google-research/bert/blob/master/run_classifier.py) file for converting examples to features :-\n",
    "\n",
    "\n",
    "1.   `convert_single_example` :- Converts a single `InputExample` into a single `InputFeatures`.\n",
    "2.   `file_based_convert_examples_to_features` :- Convert a set of `InputExamples` to a TF_Record file.\n",
    "\n",
    "For more details observe outputs for below cells\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GMeF2pc7igA"
   },
   "outputs": [],
   "source": [
    "# Instantiate an instance of QQPProcessor and tokenizer\n",
    "processor = QQPProcessor()\n",
    "label_list = processor.get_labels()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1278
    },
    "colab_type": "code",
    "id": "OdMc4HkJ7ljr",
    "outputId": "4c2ee1db-0da1-4ca2-dd03-7865563bcb37"
   },
   "outputs": [],
   "source": [
    "# Converting training examples to features\n",
    "print(\"################  Processing Training Data #####################\")\n",
    "TRAIN_TF_RECORD = os.path.join(OUTPUT_DIR, \"train.tf_record\")\n",
    "train_examples = processor.get_train_examples(TASK_DATA_DIR)\n",
    "num_train_examples = len(train_examples)\n",
    "num_train_steps = int( num_train_examples / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "run_classifier.file_based_convert_examples_to_features(train_examples, label_list, MAX_SEQ_LENGTH, tokenizer, TRAIN_TF_RECORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uq8zO7O5Dnq"
   },
   "source": [
    "## Creating Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_aUbDJA1N7w"
   },
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  # Bert Model instant \n",
    "  model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "  # Getting output for last layer of BERT\n",
    "  output_layer = model.get_pooled_output()\n",
    "  \n",
    "  # Number of outputs for last layer\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "  \n",
    "  # We will use one layer on top of BERT pretrained for creating classification model\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "    if is_training:\n",
    "      # 0.1 dropout\n",
    "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "    \n",
    "    # Calcaulte prediction probabilites and loss\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "    return (loss, per_example_loss, logits, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxTo8jgbuRoG"
   },
   "source": [
    "## Model Function Builder for Estimator\n",
    "\n",
    "Based on mode, We will create optimizer for training, evaluation metrics for evalution and estimator spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "An2DFEqX2yDJ"
   },
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  \n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    # reading features input\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "    is_real_example = None\n",
    "    if \"is_real_example\" in features:\n",
    "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "    else:\n",
    "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "    \n",
    "    # checking if training mode\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # create simple classification model\n",
    "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "        num_labels, use_one_hot_embeddings)\n",
    "    \n",
    "    # getting variables for intialization and using pretrained init checkpoint\n",
    "    tvars = tf.trainable_variables()\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      # defining optimizar function\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "      \n",
    "      # Training estimator spec\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "      # accuracy, loss, auc, F1, precision and recall metrics for evaluation\n",
    "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
    "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "        accuracy = tf.metrics.accuracy(\n",
    "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predictions)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predictions)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predictions)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predictions) \n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"eval_loss\": loss,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        }\n",
    "\n",
    "      eval_metrics = (metric_fn,\n",
    "                      [per_example_loss, label_ids, logits, is_real_example])\n",
    "      # estimator spec for evalaution\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          eval_metrics=eval_metrics,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      # estimator spec for predictions\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions={\"probabilities\": probabilities},\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elGbiKDlamy6"
   },
   "source": [
    "## Creating TPUEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ZPuYpqW97vMf",
    "outputId": "cf38c1db-a1bf-4ea8-e006-2d5a0dc7dbab"
   },
   "outputs": [],
   "source": [
    "# Define TPU configs\n",
    "if USE_TPU:\n",
    "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
    "else:\n",
    "  tpu_cluster_resolver = None\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5qq6yfS7yOw"
   },
   "outputs": [],
   "source": [
    "# create model function for estimator using model function builder\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=INIT_CHECKPOINT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=USE_TPU,\n",
    "    use_one_hot_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "T6D-ZzhlfeGx",
    "outputId": "f6ff8829-b75b-4c04-b4de-dff9c05969ff"
   },
   "outputs": [],
   "source": [
    "# Defining TPU Estimator\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=USE_TPU,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    predict_batch_size=PREDICT_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1eH8gUIZ9gD"
   },
   "source": [
    "## Finetune Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3641
    },
    "colab_type": "code",
    "id": "Rk4PXAdnjW_N",
    "outputId": "85ee1d1f-1bcd-4a00-a778-e91db6ba2ab5"
   },
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "print('QQP on BERT base model normally takes about 1 hour on TPU and 15-20 hours on GPU. Please wait...')\n",
    "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(num_train_examples))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "# we are using `file_based_input_fn_builder` for creating input function from TF_RECORD file\n",
    "train_input_fn = run_classifier.file_based_input_fn_builder(TRAIN_TF_RECORD,\n",
    "                                                            seq_length=MAX_SEQ_LENGTH,\n",
    "                                                            is_training=True,\n",
    "                                                            drop_remainder=True)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcSOEcxdZo3B"
   },
   "source": [
    "## Evalute FineTuned model\n",
    "First we will evalute on Train set and Then on Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "ne6yR18I3T09",
    "outputId": "38b1e414-6d46-471f-d2f1-07010ce3fae0"
   },
   "outputs": [],
   "source": [
    "# eval the model on train set.\n",
    "print('***** Started Train Set evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(num_train_examples))\n",
    "print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
    "# eval input function for train set\n",
    "train_eval_input_fn = run_classifier.file_based_input_fn_builder(TRAIN_TF_RECORD,\n",
    "                                                           seq_length=MAX_SEQ_LENGTH,\n",
    "                                                           is_training=False,\n",
    "                                                           drop_remainder=True)\n",
    "# evalute on train set\n",
    "result = estimator.evaluate(input_fn=train_eval_input_fn, \n",
    "                            steps=int(num_train_examples/EVAL_BATCH_SIZE))\n",
    "print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "print(\"***** Eval results *****\")\n",
    "for key in sorted(result.keys()):\n",
    "  print('  {} = {}'.format(key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "CAZ_611y7owV",
    "outputId": "e96efaf0-7699-410a-b77c-0c42db0c4c17"
   },
   "outputs": [],
   "source": [
    "# Converting eval examples to features\n",
    "print(\"################  Processing Dev Data #####################\")\n",
    "EVAL_TF_RECORD = os.path.join(OUTPUT_DIR, \"eval.tf_record\")\n",
    "eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n",
    "num_eval_examples = len(eval_examples)\n",
    "run_classifier.file_based_convert_examples_to_features(eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer, EVAL_TF_RECORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "rTT5RTAlkCO5",
    "outputId": "bbab37cf-c319-426e-bd21-85a9aa57589a"
   },
   "outputs": [],
   "source": [
    "# Eval the model on Dev set.\n",
    "print('***** Started Dev Set evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(num_eval_examples))\n",
    "print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
    "\n",
    "# eval input function for dev set\n",
    "eval_input_fn = run_classifier.file_based_input_fn_builder(EVAL_TF_RECORD,\n",
    "                                                           seq_length=MAX_SEQ_LENGTH,\n",
    "                                                           is_training=False,\n",
    "                                                           drop_remainder=True)\n",
    "# evalute on dev set\n",
    "result = estimator.evaluate(input_fn=eval_input_fn, steps=int(num_eval_examples/EVAL_BATCH_SIZE))\n",
    "print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
    "print(\"***** Eval results *****\")\n",
    "for key in sorted(result.keys()):\n",
    "  print('  {} = {}'.format(key, str(result[key])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT FineTuning Quora Question Pairs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
