{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv1d-dropout (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHxlw1TRwFSO",
        "colab_type": "text"
      },
      "source": [
        "# MaLSTM on Kaggle's Quora Question Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkkSEcrXwFSQ",
        "colab_type": "text"
      },
      "source": [
        "This notebook is about implementing the MaLSTM model (http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf) on Kaggle's Quora Question Pairs data.\n",
        "\n",
        "Blog post containing a broader explanation about the network can be found in the following link https://medium.com/@eliorcohen/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2eJQ5frLvl",
        "colab_type": "code",
        "outputId": "d140a921-e91e-4884-d85c-8006df0c6929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyzliIpmwFSR",
        "colab_type": "text"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Saxu66RwFSS",
        "colab_type": "text"
      },
      "source": [
        "First, lets import all the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DqdGHKpxwFSV",
        "colab_type": "code",
        "outputId": "eba14332-e71f-47d7-c4ee-5ae0aa888163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fbYGZr8ywFSY",
        "colab_type": "code",
        "outputId": "38fb59aa-f3a3-443e-8da8-9459c7b92521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-24 18:41:11--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.236.181\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.236.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  41.7MB/s    in 93s     \n",
            "\n",
            "2019-04-24 18:42:49 (16.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7eBGoKRswFSb",
        "colab_type": "code",
        "outputId": "e6d4d2a9-9a7f-49b6-ff5a-6d8e1098c7fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleNews-vectors-negative300.bin.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRZGq87swFSe",
        "colab_type": "text"
      },
      "source": [
        "Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-zlntz3QwFSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File paths\n",
        "TRAIN_CSV = '/gdrive/My Drive/quora/train.csv'\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "MODEL_SAVING_DIR = '/gdrive/My Drive/quora/drop'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vol2mw9GwoJh",
        "colab_type": "code",
        "outputId": "68e4d784-88db-4905-af20-9aa4fc88c5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngiya8RZwFSi",
        "colab_type": "text"
      },
      "source": [
        "Create embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IgO9Ueb6wFSk",
        "colab_type": "code",
        "outputId": "c61fd293-5d12-4164-b46b-ee969cbaa7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "train_df,test_df= train_test_split(train_df,test_size=0.2,shuffle=train_df.is_duplicate.values)\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Prepare embedding\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "questions_cols = ['question1', 'question2']\n",
        "\n",
        "# Iterate over the questions only of both training and test datasets\n",
        "for dataset in [train_df, test_df]:\n",
        "    for index, row in dataset.iterrows():\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in questions_cols:\n",
        "\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "\n",
        "                # Check for unwanted words\n",
        "                if word in stops and word not in word2vec.vocab:\n",
        "                    continue\n",
        "\n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(inverse_vocabulary)\n",
        "                    q2n.append(len(inverse_vocabulary))\n",
        "                    inverse_vocabulary.append(word)\n",
        "                else:\n",
        "                    q2n.append(vocabulary[word])\n",
        "\n",
        "            # Replace questions as word to question as number representation\n",
        "            dataset.set_value(index, question, q2n)\n",
        "            \n",
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embeddings[index] = word2vec.word_vec(word)\n",
        "\n",
        "del word2vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atxhZGERwFSt",
        "colab_type": "text"
      },
      "source": [
        "Prepare training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4ilNP7FywFSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max(),\n",
        "                     test_df.question1.map(lambda x: len(x)).max(),\n",
        "                     test_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "# Split to train validation\n",
        "validation_size = 40000\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[questions_cols]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = X,test_df[questions_cols],Y,test_df['is_duplicate']#train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
        "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
        "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrINY9QewFSw",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1gkhvjDewFSx",
        "colab_type": "code",
        "outputId": "fbcc0a58-e37c-4602-bef5-44b78f3bd50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/4b/20980621869bc2b1a06b3add3add81e4334876d35db6a6f1ecfe1c812ca6/keras-self-attention-0.40.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.16.3)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.7)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/38/4f/8f/78bd42dd2d458c5a2ad51d52f3025895463f1989a842b78362\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.40.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XYkf8mTawFS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## with attention network\n",
        "\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "# from keras.layers import Dense\n",
        "# from keras.layers import TimeDistributed,Flatten\n",
        "\n",
        "# # Model variables\n",
        "# n_hidden = 50\n",
        "# gradient_clipping_norm = 1.25\n",
        "# batch_size = 64\n",
        "# n_epoch = 25\n",
        "\n",
        "# def exponent_neg_manhattan_distance(left, right):\n",
        "#     ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "#     return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# # The visible layer\n",
        "# left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "# right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "# embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# # Embedded version of the inputs\n",
        "# encoded_left = embedding_layer(left_input)\n",
        "# encoded_right = embedding_layer(right_input)\n",
        "\n",
        "\n",
        "# # Since this is a siamese network, both sides share the same LSTM\n",
        "# #shared_lstm = LSTM(n_hidden,return_sequences=True)\n",
        "\n",
        "# #left_output = shared_lstm(encoded_left)\n",
        "# #right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "# seq = SeqSelfAttention(attention_activation='sigmoid',attention_width=10)\n",
        "\n",
        "\n",
        "# left_output = seq(encoded_left)\n",
        "# right_output = seq(encoded_right)\n",
        "\n",
        "\n",
        "# seq2 = SeqSelfAttention(attention_activation='sigmoid',attention_width=5)\n",
        "\n",
        "# left_output = seq2(left_output)\n",
        "# right_output = seq2(right_output)\n",
        "\n",
        "# dense = TimeDistributed(Dense(5))\n",
        "\n",
        "# left_output = dense(left_output)\n",
        "# right_output = dense(right_output)\n",
        "\n",
        "# flat = Flatten()\n",
        "\n",
        "# left_output = flat(left_output)\n",
        "# right_output = flat(right_output)\n",
        "\n",
        "\n",
        "# # Calculates the distance as defined by the MaLSTM model\n",
        "# malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# # Pack it all up into a model\n",
        "# malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# # Adadelta optimizer, with gradient clipping by norm\n",
        "# optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "# malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# # Start training\n",
        "# training_start_time = time()\n",
        "\n",
        "# malstm.summary()\n",
        "# malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "#                             validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "# print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfsHc578wFS3",
        "colab_type": "text"
      },
      "source": [
        "Plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gw6PuDb7wFS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot accuracy\n",
        "# plt.plot(malstm_trained.history['acc'])\n",
        "# plt.plot(malstm_trained.history['val_acc'])\n",
        "# plt.title('Model Accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot loss\n",
        "# plt.plot(malstm_trained.history['loss'])\n",
        "# plt.plot(malstm_trained.history['val_loss'])\n",
        "# plt.title('Model Loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TJCpyqx_wFS5",
        "colab_type": "code",
        "outputId": "0012f226-1d70-494b-b352-278b5c853835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        }
      },
      "source": [
        "## with attention network\n",
        "\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed,Flatten,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Dropout,LSTM\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Model variables\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 128\n",
        "n_epoch = 50\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "#shared_lstm = LSTM(n_hidden,return_sequences=True)\n",
        "\n",
        "#left_output = shared_lstm(encoded_left)\n",
        "#right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "\n",
        "\n",
        "# dense = LSTM(2000,return_sequences=True)\n",
        "# encoded_left = dense(encoded_left)\n",
        "# encoded_right = dense(encoded_right)\n",
        "\n",
        "\n",
        "## conv12\n",
        "conv = Conv1D(filters=500, kernel_size=4, padding='valid', activation='relu', strides=1)\n",
        "encoded_left = conv(encoded_left)\n",
        "encoded_right = conv(encoded_right)\n",
        "\n",
        "drop = Dropout(0.2)\n",
        "encoded_left = drop(encoded_left)\n",
        "encoded_right = drop(encoded_right)\n",
        "# pooling  = MaxPooling1D(pool_size=4)\n",
        "# encoded_left = pooling(encoded_left)\n",
        "# encoded_right = pooling(encoded_right)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "conv2 = Conv1D(filters=1000, kernel_size=4, padding='valid', activation='relu', strides=1)\n",
        "encoded_left = conv2(encoded_left)\n",
        "encoded_right = conv2(encoded_right)\n",
        "drop = Dropout(0.2)\n",
        "encoded_left = drop(encoded_left)\n",
        "encoded_right = drop(encoded_right)\n",
        "# pooling  = MaxPooling1D(pool_size=4)\n",
        "# encoded_left = pooling(encoded_left)\n",
        "# encoded_right = pooling(encoded_right)\n",
        "\n",
        "\n",
        "attention = SeqSelfAttention(attention_width=10,\n",
        "    attention_activation='relu',attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL)\n",
        "encoded_left = attention(encoded_left)\n",
        "encoded_right = attention(encoded_right)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# pooling2  = Flatten()\n",
        "# encoded_left = pooling2(encoded_left)\n",
        "# encoded_right = pooling2(encoded_right)\n",
        "\n",
        "\n",
        "dense = Dense(500,activation='relu')\n",
        "left_output = dense(encoded_left)\n",
        "right_output = dense(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# Pack it all up into a model\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "#optimizer = Adam(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "malstm.summary()\n",
        "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_17 (InputLayer)           (None, 212)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           (None, 212)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 212, 300)     25800600    input_17[0][0]                   \n",
            "                                                                 input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 209, 500)     600500      embedding_9[2][0]                \n",
            "                                                                 embedding_9[3][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 209, 500)     0           conv1d_17[0][0]                  \n",
            "                                                                 conv1d_17[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 206, 1000)    2001000     dropout_17[0][0]                 \n",
            "                                                                 dropout_17[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 206, 1000)    0           conv1d_18[0][0]                  \n",
            "                                                                 conv1d_18[1][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "seq_self_attention_8 (SeqSelfAt (None, 206, 1000)    1000001     dropout_18[0][0]                 \n",
            "                                                                 dropout_18[1][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 206, 500)     500500      seq_self_attention_8[0][0]       \n",
            "                                                                 seq_self_attention_8[1][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 1)            0           dense_9[0][0]                    \n",
            "                                                                 dense_9[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 29,902,601\n",
            "Trainable params: 4,102,001\n",
            "Non-trainable params: 25,800,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 323432 samples, validate on 80858 samples\n",
            "Epoch 1/50\n",
            "323432/323432 [==============================] - 1197s 4ms/step - loss: 0.2412 - acc: 0.6243 - val_loss: 0.5875 - val_acc: 0.3691\n",
            "Epoch 2/50\n",
            "  6272/323432 [..............................] - ETA: 18:00 - loss: 0.2374 - acc: 0.6300"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEa89xlTrI5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XYnB1-tGwFS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "## add cosine similarity loss\n",
        "\n",
        "## with attention network\n",
        "\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed,Flatten,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Dropout,Dot\n",
        "\n",
        "# Model variables\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 128\n",
        "n_epoch = 25\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "#shared_lstm = LSTM(n_hidden,return_sequences=True)\n",
        "\n",
        "#left_output = shared_lstm(encoded_left)\n",
        "#right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "## conv12\n",
        "conv = Conv1D(filters=1500, kernel_size=4, padding='valid', activation='sigmoid', strides=1)\n",
        "encoded_left = conv(encoded_left)\n",
        "encoded_right = conv(encoded_right)\n",
        "\n",
        "drop = Dropout(0.3)\n",
        "encoded_left = drop(encoded_left)\n",
        "encoded_right = drop(encoded_right)\n",
        "\n",
        "pooling  = MaxPooling1D(pool_size=4)\n",
        "encoded_left = pooling(encoded_left)\n",
        "encoded_right = pooling(encoded_right)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "conv2 = Conv1D(filters=1500, kernel_size=4, padding='valid', activation='sigmoid', strides=1)\n",
        "encoded_left = conv2(encoded_left)\n",
        "encoded_right = conv2(encoded_right)\n",
        "\n",
        "drop = Dropout(0.3)\n",
        "encoded_left = drop(encoded_left)\n",
        "encoded_right = drop(encoded_right)\n",
        "\n",
        "pooling2  = GlobalMaxPooling1D()\n",
        "encoded_left = pooling2(encoded_left)\n",
        "encoded_right = pooling2(encoded_right)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dense = Dense(256)\n",
        "left_outpu = dense(encoded_left)\n",
        "right_output = dense(encoded_right)\n",
        "\n",
        "\n",
        "dense = Dense(256)\n",
        "left_output = dense(encoded_left)\n",
        "right_output = dense(encoded_right)\n",
        "\n",
        "\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "#malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "malstm_distance = Dot(-1,normalize=True)([left_output,right_output])\n",
        "# Pack it all up into a model\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "malstm.summary()\n",
        "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y1EiOXbBwFS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}