{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conv1d.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTggsa6FqzQO",
        "colab_type": "text"
      },
      "source": [
        "# MaLSTM on Kaggle's Quora Question Pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cEu1rxPq6y1",
        "colab_type": "code",
        "outputId": "00b7914a-e23b-42a9-d585-0afbebbf4053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#!pip install Keras==2.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN5P2HKKqzQQ",
        "colab_type": "text"
      },
      "source": [
        "This notebook is about implementing the MaLSTM model (http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf) on Kaggle's Quora Question Pairs data.\n",
        "\n",
        "Blog post containing a broader explanation about the network can be found in the following link https://medium.com/@eliorcohen/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zni6JztIqzQR",
        "colab_type": "text"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdmXOtxHqzQS",
        "colab_type": "text"
      },
      "source": [
        "First, lets import all the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "njgN4wwIqzQT",
        "colab_type": "code",
        "outputId": "4580a274-98a9-4c10-cc41-ac0677f1ff9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import datetime\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IJg7uGTmqzQY",
        "colab_type": "code",
        "outputId": "0a261912-173c-4491-df4d-38b9af573792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-30 02:20:59--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.138.125\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.138.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VKqIJOhZqzQe",
        "colab_type": "code",
        "outputId": "5ac4cd3d-bb32-4096-a0ae-13c0f4f13b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!ls /gdrive/My\\ Drive/quora"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " drop\t\t\t\t    weights-improvement-04-0.76.hdf5\n",
            " lda\t\t\t\t    weights-improvement-04-0.80.hdf5\n",
            "'test_processed (1).csv'\t    weights-improvement-05-0.77.hdf5\n",
            " test_processed.csv\t\t    weights-improvement-05-0.80.hdf5\n",
            " train.csv\t\t\t    weights-improvement-05-0.81.hdf5\n",
            "'train_processed (1).csv'\t    weights-improvement-06-0.77.hdf5\n",
            " train_processed.csv\t\t    weights-improvement-06-0.81.hdf5\n",
            " vocab.txt\t\t\t    weights-improvement-07-0.78.hdf5\n",
            " weights-improvement-01-0.72.hdf5   weights-improvement-07-0.81.hdf5\n",
            " weights-improvement-01-0.76.hdf5   weights-improvement-08-0.81.hdf5\n",
            " weights-improvement-01-0.77.hdf5   weights-improvement-08-0.82.hdf5\n",
            " weights-improvement-02-0.74.hdf5   weights-improvement-09-0.81.hdf5\n",
            " weights-improvement-02-0.78.hdf5   weights-improvement-09-0.82.hdf5\n",
            " weights-improvement-02-0.79.hdf5   weights-improvement-11-0.82.hdf5\n",
            " weights-improvement-03-0.76.hdf5   weights-improvement-14-0.82.hdf5\n",
            " weights-improvement-03-0.79.hdf5   wemb\n",
            " weights-improvement-03-0.80.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0opvEKIqzQl",
        "colab_type": "text"
      },
      "source": [
        "Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kCCereKfqzQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File paths\n",
        "TRAIN_CSV = '/gdrive/My Drive/quora/train.csv'\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "MODEL_SAVING_DIR = '/gdrive/My Drive/quora'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWTwfxqgqzQq",
        "colab_type": "text"
      },
      "source": [
        "Create embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xkqjw8UJqzQs",
        "colab_type": "code",
        "outputId": "3c218c70-0974-4a73-a5c0-8717665c74c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load training and test set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "train_df,test_df= train_test_split(train_df,test_size=0.2,shuffle=train_df.is_duplicate.values)\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    ''' Pre process and convert texts to a list of words '''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "   \n",
        "    return text\n",
        "\n",
        "# Prepare embedding\n",
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "questions_cols = ['question1', 'question2']\n",
        "import pickle as pk\n",
        "# Iterate over the questions only of both training and test datasets\n",
        "for dataset in [train_df, test_df]:\n",
        "    for index, row in dataset.iterrows():\n",
        "   \n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in questions_cols:\n",
        "\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "\n",
        "                # Check for unwanted words\n",
        "                if word in stops and word not in word2vec.vocab:\n",
        "                    continue\n",
        "\n",
        "                if word not in vocabulary:\n",
        "                    vocabulary[word] = len(inverse_vocabulary)\n",
        "                    q2n.append(len(inverse_vocabulary))\n",
        "                    inverse_vocabulary.append(word)\n",
        "                else:\n",
        "                    q2n.append(vocabulary[word])\n",
        "\n",
        "            # Replace questions as word to question as number representation\n",
        "            dataset.set_value(index, question, q2n)\n",
        "\n",
        "with open(MODEL_SAVING_DIR+'/vocab.txt','wb') as f:\n",
        "  pk.dump(vocabulary,f)\n",
        "  \n",
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embeddings[index] = word2vec.word_vec(word)\n",
        "\n",
        "del word2vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISfKLz7CqzQw",
        "colab_type": "text"
      },
      "source": [
        "Prepare training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DBm8i7RTqzQy",
        "colab_type": "code",
        "outputId": "3c028004-cf28-4097-f825-aa2763868ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_seq_length = max(train_df.question1.map(lambda x: len(x)).max(),\n",
        "                     train_df.question2.map(lambda x: len(x)).max(),\n",
        "                     test_df.question1.map(lambda x: len(x)).max(),\n",
        "                     test_df.question2.map(lambda x: len(x)).max())\n",
        "\n",
        "print('max_seq={}'.format(max_seq_length))\n",
        "# Split to train validation\n",
        "validation_size = 40000\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[questions_cols]\n",
        "Y = train_df['is_duplicate']\n",
        "\n",
        "X_train, X_validation, Y_train, Y_validation = X,test_df[questions_cols],Y,test_df['is_duplicate']#train_test_split(X, Y, test_size=validation_size)\n",
        "\n",
        "# Split to dicts\n",
        "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
        "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
        "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "Y_train = Y_train.values\n",
        "Y_validation = Y_validation.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train['left'].shape == X_train['right'].shape\n",
        "assert len(X_train['left']) == len(Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_seq=212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzefnIaIqzQ1",
        "colab_type": "text"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Wn7UOsRmqzQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "quUj6f5yqzQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## with attention network\n",
        "\n",
        "# from keras_self_attention import SeqSelfAttention\n",
        "# from keras.layers import Dense\n",
        "# from keras.layers import TimeDistributed,Flatten\n",
        "\n",
        "# # Model variables\n",
        "# n_hidden = 50\n",
        "# gradient_clipping_norm = 1.25\n",
        "# batch_size = 64\n",
        "# n_epoch = 25\n",
        "\n",
        "# def exponent_neg_manhattan_distance(left, right):\n",
        "#     ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "#     return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# # The visible layer\n",
        "# left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "# right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "# embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# # Embedded version of the inputs\n",
        "# encoded_left = embedding_layer(left_input)\n",
        "# encoded_right = embedding_layer(right_input)\n",
        "\n",
        "\n",
        "# # Since this is a siamese network, both sides share the same LSTM\n",
        "# #shared_lstm = LSTM(n_hidden,return_sequences=True)\n",
        "\n",
        "# #left_output = shared_lstm(encoded_left)\n",
        "# #right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "# seq = SeqSelfAttention(attention_activation='sigmoid',attention_width=10)\n",
        "\n",
        "\n",
        "# left_output = seq(encoded_left)\n",
        "# right_output = seq(encoded_right)\n",
        "\n",
        "\n",
        "# seq2 = SeqSelfAttention(attention_activation='sigmoid',attention_width=5)\n",
        "\n",
        "# left_output = seq2(left_output)\n",
        "# right_output = seq2(right_output)\n",
        "\n",
        "# dense = TimeDistributed(Dense(5))\n",
        "\n",
        "# left_output = dense(left_output)\n",
        "# right_output = dense(right_output)\n",
        "\n",
        "# flat = Flatten()\n",
        "\n",
        "# left_output = flat(left_output)\n",
        "# right_output = flat(right_output)\n",
        "\n",
        "\n",
        "# # Calculates the distance as defined by the MaLSTM model\n",
        "# malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# # Pack it all up into a model\n",
        "# malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# # Adadelta optimizer, with gradient clipping by norm\n",
        "# optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "# malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# # Start training\n",
        "# training_start_time = time()\n",
        "\n",
        "# malstm.summary()\n",
        "# malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "#                             validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
        "\n",
        "# print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AImPRczRqzRB",
        "colab_type": "text"
      },
      "source": [
        "Plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gMGUXViOqzRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot accuracy\n",
        "# plt.plot(malstm_trained.history['acc'])\n",
        "# plt.plot(malstm_trained.history['val_acc'])\n",
        "# plt.title('Model Accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot loss\n",
        "# plt.plot(malstm_trained.history['loss'])\n",
        "# plt.plot(malstm_trained.history['val_loss'])\n",
        "# plt.title('Model Loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fy2034MYqzRL",
        "colab_type": "code",
        "outputId": "1e82f5b9-6488-4f5c-bff6-75ac72f03e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "## with attention network\n",
        "\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import Dense\n",
        "from keras.layers import TimeDistributed,Flatten,Conv1D,MaxPooling1D,GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# Model variables\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 128\n",
        "n_epoch = 100\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "#shared_lstm = LSTM(n_hidden,return_sequences=True)\n",
        "\n",
        "#left_output = shared_lstm(encoded_left)\n",
        "#right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "## conv12\n",
        "conv = Conv1D(filters=1500, kernel_size=4, padding='valid', activation='sigmoid', strides=1)\n",
        "\n",
        "\n",
        "encoded_left = conv(encoded_left)\n",
        "encoded_right = conv(encoded_right)\n",
        "\n",
        "pooling  = MaxPooling1D(pool_size=4)\n",
        "encoded_left = pooling(encoded_left)\n",
        "encoded_right = pooling(encoded_right)\n",
        "\n",
        "conv2 = Conv1D(filters=3000, kernel_size=4, padding='valid', activation='sigmoid', strides=1)\n",
        "encoded_left = conv2(encoded_left)\n",
        "encoded_right = conv2(encoded_right)\n",
        "\n",
        "\n",
        "pooling2  = GlobalMaxPooling1D()\n",
        "encoded_left = pooling2(encoded_left)\n",
        "encoded_right = pooling2(encoded_right)\n",
        "\n",
        "\n",
        "dense = Dense(256)\n",
        "left_output = dense(encoded_left)\n",
        "right_output = dense(encoded_right)\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# Pack it all up into a model\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "filepath=MODEL_SAVING_DIR+\"/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "malstm.summary()\n",
        "#malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "                            #validation_data=([X_validation['left'], X_validation['right']], Y_validation),callbacks=callbacks_list)\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 212)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 212)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 212, 300)     25800600    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 209, 1500)    1801500     embedding_1[2][0]                \n",
            "                                                                 embedding_1[3][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 52, 1500)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_1[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 49, 3000)     18003000    max_pooling1d_1[0][0]            \n",
            "                                                                 max_pooling1d_1[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 3000)         0           conv1d_2[0][0]                   \n",
            "                                                                 conv1d_2[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          768256      global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_1[1][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1)            0           dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 46,373,356\n",
            "Trainable params: 20,572,756\n",
            "Non-trainable params: 25,800,600\n",
            "__________________________________________________________________________________________________\n",
            "Training time finished.\n",
            "100 epochs in 0:00:00.005378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2zJHY3ILqzRO",
        "colab_type": "code",
        "outputId": "9b4f221a-cebf-4794-b077-f8d7ee21fc8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# \n",
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "filepath_latest = MODEL_SAVING_DIR+'/weights-improvement-14-0.82.hdf5'\n",
        "malstm.load_weights(filepath_latest)\n",
        "print(filepath_latest)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/quora/weights-improvement-14-0.82.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsQoVhl0P_XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras --upgrade\n",
        "!pip install tensorflow-gpu --upgrade\n",
        "!ls /gdrive/My\\ Drive/quora/\n",
        "!cp /gdrive/My\\ Drive/quora/weights-improvement-01-0.77.hdf5 .\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHGI_RVljm7w",
        "colab_type": "code",
        "outputId": "20102390-1b2e-4653-d441-e8aaaa8ec1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
        "#                             validation_data=([X_validation['left'], X_validation['right']], Y_validation),callbacks=callbacks_list)\n",
        "malstm.predict([X_validation['left'][0:10], X_validation['right'][0:10]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21023595],\n",
              "       [1.        ],\n",
              "       [0.2981399 ],\n",
              "       [0.07219958],\n",
              "       [0.05977337],\n",
              "       [0.6075143 ],\n",
              "       [0.15939113],\n",
              "       [0.08044683],\n",
              "       [0.2218177 ],\n",
              "       [0.48542237]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up38jX9ZF1xQ",
        "colab_type": "code",
        "outputId": "8f0a2249-f14f-4912-fc3b-20d610d89988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_validation['left'].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80858, 212)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRLUUT-hkOvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -c 'import keras; print(keras.__version__)'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le5rraxlUhu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}